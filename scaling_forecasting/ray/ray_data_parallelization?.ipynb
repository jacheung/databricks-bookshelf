{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19042c4f-f4d5-4e17-8030-cdce13849787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qU databricks-feature-engineering mlflow ray[default] ray[data] databricks-sql-connector\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "648b7a92-5225-4115-8521-df6cc892bbae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"jon_cheung\"\n",
    "schema = \"ray_gtm_examples\"\n",
    "table = \"data_synthetic_timeseries_mini\"\n",
    "label=\"y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "619cb71f-f3f8-4b48-87cc-9c4050665b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optional: Generate a massive time-series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba0d28a-03b7-428d-ab0e-bc7babf504da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./generate_timeseries_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccf949cc-893d-4248-bc63-e49bf38b78c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Synthetic data generation \n",
    "import pandas as pd\n",
    "\n",
    "if not spark.catalog.tableExists(f\"{catalog}.{schema}.{table}\"): \n",
    "  # Create table for features\n",
    "  id_sdf.write.mode('overwrite').saveAsTable(f\"{catalog}.{schema}.{table}\")\n",
    "  print(f\"... OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfc4378b-3c77-436d-9955-dc202085a6bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ray Data with `map_groups`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14052039-5490-4495-9dbf-35a4319b8f90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "# The recommended configuration for a Ray cluster is as follows:\n",
    "# - set the num_cpus_per_node to the CPU count per worker node (with this configuration, each Apache Spark worker node launches one Ray worker node that will fully utilize the resources of each Apache Spark worker node.)\n",
    "# - set min_worker_nodes to the number of Ray worker nodes you want to launch on each node.\n",
    "# - set max_worker_nodes to the total amount of worker nodes (this and `min_worker_nodes` together enable autoscaling)\n",
    "setup_ray_cluster(\n",
    "  min_worker_nodes=2,\n",
    "  max_worker_nodes=8,\n",
    "  num_cpus_per_node=16,\n",
    "  num_gpus_worker_node=0,\n",
    "  collect_log_to_path=\"/dbfs/Users/jon.cheung@databricks.com/ray_collected_logs\",\n",
    "  RAY_memory_monitor_refresh_ms=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b80d9d07-d44b-4813-a40b-3a2631e80d9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# create volume for Ray Data \n",
    "os.environ['RAY_UC_VOLUMES_FUSE_TEMP_DIR'] = f'/Volumes/{catalog}/{schema}/ray_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c8549b-83a6-40cd-aec6-b329c19a1b2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import os\n",
    "from mlflow.utils.databricks_utils import get_databricks_env_vars\n",
    "from ray.data import from_spark\n",
    "\n",
    "experiment_name = '/Users/jon.cheung@databricks.com/ray_prophet_large'\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow_db_creds = get_databricks_env_vars(\"databricks\")\n",
    "\n",
    "def train_and_inference_prophet(grouped_data:pd.DataFrame, \n",
    "                                horizon:int,\n",
    "                                parent_run_id:str\n",
    "                                ):\n",
    "        # Set mlflow credentials and active MLflow experiment within each Ray task\n",
    "        os.environ.update(mlflow_db_creds)\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "        # Create nested child runs named after the group\n",
    "        group_name = grouped_data.loc[0,'group_name']\n",
    "        with mlflow.start_run(run_name = f\"{group_name}\",\n",
    "                              parent_run_id=parent_run_id):\n",
    "          dataset = mlflow.data.from_pandas(grouped_data)\n",
    "          mlflow.log_input(dataset)\n",
    "                  \n",
    "          m = Prophet(daily_seasonality=True)\n",
    "          m.fit(grouped_data)\n",
    "          future = m.make_future_dataframe(periods=horizon)\n",
    "          forecast = m.predict(future)\n",
    "        return forecast\n",
    "\n",
    "ray_data = from_spark(spark.read.table(f\"{catalog}.{schema}.{table}\"), \n",
    "                      use_spark_chunk_api=False)\n",
    "with mlflow.start_run(run_name=\"prophet_models_250121_mini_autoscale\") as parent_run: \n",
    "  grouped = ray_data.groupby(\"group_name\")\n",
    "  grouped.map_groups(train_and_inference_prophet, \n",
    "              num_cpus=1,\n",
    "              fn_kwargs={\"horizon\": 14,\n",
    "                         \"parent_run_id\": parent_run.info.run_id})\n",
    "  \n",
    "  # # # map_groups() operation is lazy, meaning it won't be executed until you call an action or convert it to another format. This allows Ray to optimize the execution plan for better performance.\n",
    "  # results = grouped.take_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d1e0139-a0e3-45e1-821c-0a5698aae5d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "ray_data_parallelization?",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
