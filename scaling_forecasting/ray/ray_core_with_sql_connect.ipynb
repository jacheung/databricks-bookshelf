{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19042c4f-f4d5-4e17-8030-cdce13849787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qU databricks-feature-engineering mlflow ray[default] ray[data] databricks-sql-connector\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "648b7a92-5225-4115-8521-df6cc892bbae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"jon_cheung\"\n",
    "schema = \"ray_gtm_examples\"\n",
    "table = \"data_synthetic_timeseries_mini\"\n",
    "label=\"y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "619cb71f-f3f8-4b48-87cc-9c4050665b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optional: Generate a massive time-series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba0d28a-03b7-428d-ab0e-bc7babf504da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./generate_timeseries_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccf949cc-893d-4248-bc63-e49bf38b78c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Synthetic data generation \n",
    "import pandas as pd\n",
    "\n",
    "if not spark.catalog.tableExists(f\"{catalog}.{schema}.{table}\"): \n",
    "  # Create table for features\n",
    "  id_sdf.write.mode('overwrite').saveAsTable(f\"{catalog}.{schema}.{table}\")\n",
    "  print(f\"... OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b84a43a-5af9-4dae-98ab-7ae5ba3a0220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Option 1: Ray Core with SQL Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b06f5271-68e1-4d67-83b2-c8367f092eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get all unique group names for Ray Core for loop\n",
    "uniques = spark.sql(f\"SELECT DISTINCT group_name FROM {catalog}.{schema}.{table}\").toPandas()\n",
    "group_names = list(uniques['group_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f65f4223-c7a2-451f-8266-660fd00ef859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "# The recommended configuration for a Ray cluster is as follows:\n",
    "# - set the num_cpus_per_node to the CPU count per worker node (with this configuration, each Apache Spark worker node launches one Ray worker node that will fully utilize the resources of each Apache Spark worker node.)\n",
    "# - set min_worker_nodes to the number of Ray worker nodes you want to launch on each node.\n",
    "# - set max_worker_nodes to the total amount of worker nodes (this and `min_worker_nodes` together enable autoscaling)\n",
    "setup_ray_cluster(\n",
    "  min_worker_nodes=2,\n",
    "  max_worker_nodes=8,\n",
    "  num_cpus_per_node=16,\n",
    "  num_gpus_worker_node=0,\n",
    "  collect_log_to_path=\"/dbfs/Users/jon.cheung@databricks.com/ray_collected_logs\",\n",
    "  RAY_memory_monitor_refresh_ms=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ac9c339-fcc1-4041-94f9-e95ec3ecf7ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from databricks import sql\n",
    "import pandas.io.sql as psql\n",
    "from prophet import Prophet\n",
    "import pickle\n",
    "import mlflow\n",
    "import os\n",
    "from mlflow.utils.databricks_utils import get_databricks_env_vars\n",
    "\n",
    "experiment_name = '/Users/jon.cheung@databricks.com/ray_core_prophet'\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow_db_creds = get_databricks_env_vars(\"databricks\")\n",
    "\n",
    "# Here we transform our training code to one that works with Ray. We simply add a @ray.remote decorator to the function along with some mlflow logging parameters for a nested child runs\n",
    "@ray.remote\n",
    "def train_and_inference_prophet(group_name:str, \n",
    "                                horizon:int,\n",
    "                                parent_run_id:str\n",
    "                                ):\n",
    "        # It's inefficient for each actor to inherit a sliver of the data from the head node. \n",
    "        # A better approach is to create a connection from each worker to our data, load, and train. \n",
    "        # The below three parameters are inherited from the SQL warehouse via these steps:\n",
    "        # Create an SQL warehouse (See Compute // SQL Warehouse); I used x-small for my 50k rows per model query) > Connection Details > Python\n",
    "        # I then put these values into the `init` script for my compute cluster\n",
    "        connection = sql.connect(\n",
    "                        server_hostname = os.getenv(\"DATABRICKS_SERVER_HOSTNAME\"),\n",
    "                        http_path = os.getenv(\"DATABRICKS_HTTP_PATH\"),\n",
    "                        access_token = os.getenv(\"DATABRICKS_TOKEN\"))\n",
    "        with connection.cursor() as cursor:\n",
    "            query=f\"SELECT * FROM {catalog}.{schema}.{table} WHERE group_name = '{group_name}'\"\n",
    "            selected_data = psql.read_sql(query, connection)\n",
    "\n",
    "        # Set mlflow credentials and active MLflow experiment within each Ray task\n",
    "        os.environ.update(mlflow_db_creds)\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "        with mlflow.start_run(run_name = f\"{group_name}\",\n",
    "                              parent_run_id=parent_run_id):\n",
    "\n",
    "                dataset = mlflow.data.from_pandas(selected_data)\n",
    "                mlflow.log_input(dataset)\n",
    "                \n",
    "                m = Prophet(daily_seasonality=True)\n",
    "                m.fit(selected_data)\n",
    "                future = m.make_future_dataframe(periods=horizon)\n",
    "                forecast = m.predict(future)\n",
    "                mlflow.prophet.log_model(pr_model=m,\n",
    "                                         artifact_path=\"prophet_model\")\n",
    "        return forecast\n",
    "\n",
    "# Here, the call to the train_and_inference_prophet function creates an object reference.\n",
    "# Using 8 workers (each with 64GB memory and 16 cores; i.e. m5.2xlarge on Azure), we can parallelize our training to 128 tasks in parallel. \n",
    "\n",
    "with mlflow.start_run(run_name=\"prophet_models_250121_sql_connect_1_cpu\") as parent_run: \n",
    "        # Start parent run on the main driver process\n",
    "        forecasts_obj_ref = [train_and_inference_prophet\n",
    "                        .options(num_cpus=1)\n",
    "                        .remote(group_name=group,\n",
    "                                horizon=14,\n",
    "                                parent_run_id=parent_run.info.run_id\n",
    "                                ) \n",
    "                        for group in group_names]\n",
    "\n",
    "        # We need to call ray.get() on the referenced object to create a blocking call. \n",
    "        # Blocking call is one which will not return until the action it performs is complete.\n",
    "        forecasts = ray.get(forecasts_obj_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a69bbc92-82e7-45c2-8e0d-9245bdce0c41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shutdown_ray_cluster()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "ray_core_with_sql_connect",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
